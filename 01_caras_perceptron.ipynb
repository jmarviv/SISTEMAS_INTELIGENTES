{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8vB1_Os3Ucd"
      },
      "source": [
        "# Perceptrón aplicado al corpus y tarea Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBGy1BB3Uce"
      },
      "source": [
        "### 1. Lectura del corpus\n",
        "\n",
        "Cargamos el corpus Iris y comprobamos que las matrices de datos `X` y etiquetas `y` contienen el número de filas y columnas que esperamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "6VbTWwY03Uce",
        "outputId": "399b24ae-90f9-4e09-abd5-37cc875c58ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np;\n",
        "import matplotlib.pyplot as plt;\n",
        "from sklearn.datasets import load_digits;\n",
        "digits = load_digits();\n",
        "X = digits.data.astype(np.float16);\n",
        "y = digits.target.astype(np.uint).reshape(-1, 1);\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(16, 16))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"none\")\n",
        "    ax.set_title(\"Mostra {!s}\".format(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4OwWbTm3Ucf"
      },
      "source": [
        "### 2. Partición del corpus\n",
        "\n",
        "Creamos un split del dataset iris con un $20\\%$ de datos (30 muestras) para evaluación (*test*), y el resto (120 muestras) para entrenamiento (*training*), barajando previamente los datos de acuerdo con una semilla dada para la generación de números aleatorios.\n",
        "\n",
        "Aquí, como en todo código que incluya aleatoriedad (que requiera generar números aleatorios), conviene fijar dicha semilla para poder reproducir experimentos posteriormente con exactitud. En este caso, usaremos como semilla un objeto `int` con valor 23."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-paSyOI3Ucf",
        "outputId": "abdb82ce-9738-4b6f-cb66-275c76ac92a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape = (1437, 64) ; X_test.shape = (360, 64)\n",
            "y_train.shape = (1437, 1) ; y_test.shape = (360, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# esta función realiza la partición dandole el porcentaje de datos que meteremos en el test,\n",
        "# si queremos que baraje y le podemos pasar una semilla para que baraje de manera predeterminada(random state)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "       train_test_split(X, y,\n",
        "                        test_size=0.2,\n",
        "                        shuffle=True,\n",
        "                        random_state=23)\n",
        "print(\"X_train.shape =\", X_train.shape, \"; X_test.shape =\", X_test.shape)\n",
        "print(\"y_train.shape =\", y_train.shape, \"; y_test.shape =\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWjnQc8U3Ucf"
      },
      "source": [
        "### 3. Implementación del algoritmo Perceptrón\n",
        "\n",
        "Implementación del algoritmo de aprendizaje Perceptrón, partiendo de pesos nulos.\n",
        "\n",
        "Recibe como parámetros de entrada:\n",
        "\n",
        "- Matriz de muestras de entrenamiento `X`, de tamaño $N\\times D$,\n",
        "- Matriz (vector columna) de etiquetas de clase `y`, de tamaño $N\\times 1$,\n",
        "- Valores de los hiperparámetros del algoritmo (a optimizar de manera experimental):\n",
        "    - Variable de margen `b` $\\ge 0$,\n",
        "    - Factor de aprendizaje `a` $> 0$,\n",
        "    - Número máximo de iteraciones `K` $> 0$.\n",
        "\n",
        "Devuelve:\n",
        "\n",
        "- Matriz de pesos optimizados `W`, en notación homogénea, de tamaño $(1+D)\\times C$,  \n",
        "- Número de muestras de train incorrectamente clasificadas `E` durante la última iteración realizada,\n",
        "- Número de iteraciones ejecutadas `k`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ePxxn2H_3Ucg"
      },
      "outputs": [],
      "source": [
        "def perceptron(X, y, b=0.1, a=1.0, K=200):        # @ = multiplicar matrices y devuelve un escalar\n",
        "    N, D = X.shape;                               # N: num. muestras; D: dimensionalidad\n",
        "    Y = np.unique(y);                             # Y: Conjunto etiquetas de clase\n",
        "    C = Y.size;                                   # C: número de clases\n",
        "    W = np.zeros((1+D, C));                       # Inicialización matriz de pesos nulos\n",
        "    for k in range(1, K+1):\n",
        "        E = 0                                     # E: Contador de errores de clasificación\n",
        "        for n in range(N):\n",
        "            xn = np.array([1, *X[n, :]]);         # xn: vector x en notación compacta (el *de *X[N,:] es para descommpactar la fila extraida de X, que se extrae la fila que toque segun n)\n",
        "            yn = np.squeeze(np.where(Y==y[n]));   # yn: etiq. de clase correcta en {0,...,C-1} (coge el valor que corresponda en el array de Y para usar su posición, no su nombre)\n",
        "            gn = W[:,yn] @ xn;                    # gn: valor f.discr. de la clase correcta (W_dos(traspuesta) * x_zero)\n",
        "            err = False;\n",
        "            for c in np.arange(C):\n",
        "                if c != yn:\n",
        "                    if W[:,c] @ xn + b >= gn:     # ¿Se clasifica mal la muestra xn?\n",
        "                        W[:, c] = W[:, c] - a*xn; # Ajustamos pesos de la clase incorrecta\n",
        "                        err = True;\n",
        "            if err:\n",
        "                W[:, yn] = W[:, yn] + a*xn;       # Ajustamos pesos de la clase correcta\n",
        "                E = E + 1;\n",
        "        if E == 0:                                # Algoritmo converge, finalizamos entrenamiento.\n",
        "            break;\n",
        "    return W, E, k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODpANFoz3Ucg"
      },
      "source": [
        "**Nota 1:** el carácter `*` delante de una secuencia actúa como [operador de desempaquetado (unpacking operator)]((https://peps.python.org/pep-0448/)).\n",
        "\n",
        "**Nota 2:** el [operador `@`](https://peps.python.org/pep-0465/) denota multiplicación de matrices.\n",
        "\n",
        "Conviene resaltar que **el valor `E` devuelto no nos sirve para calcular la tasa de error de train**, ya que los pesos se modifican a lo largo de la iteración. La tasa de error en train se debe calcular con unos pesos fijos (invariables) para todas las muestras, p.e. al finalizar el entrenamiento, o incluso al final de cada iteración. `E` solo nos sirve para conocer el número de actualizaciones de pesos realizadas en la última iteración del algoritmo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5iibDeu3Ucg"
      },
      "source": [
        "### 4. Aprendizaje de un clasificador lineal con Perceptrón\n",
        "\n",
        "Perceptrón devuelve una matriz de pesos optimizados $\\mathbf{W}^*$ que minimiza el número de errores de entrenamiento (con margen `b`):\n",
        "\n",
        "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{I}\\biggl(\\max_{c\\neq y_n}\\;(\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b) \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{x}_n\\biggr)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxDTZUzX3Ucg",
        "outputId": "8d330ab8-4b1a-4410-e749-cd41408c290d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de iteraciones ejecutadas:  100\n",
            "Número de errores de entrenamiento durante la última iteración:  10\n",
            "Vectores de pesos de las clases (por columnas, notación homogénea):\n",
            " [[-1.420e+02 -2.120e+02 -1.420e+02 -1.430e+02 -1.050e+02 -1.470e+02\n",
            "  -1.390e+02 -1.420e+02 -1.450e+02 -1.900e+02]\n",
            " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
            " [-1.700e+01 -2.600e+01  3.300e+01  5.000e+01 -3.500e+01 -1.600e+01\n",
            "  -4.200e+01  4.100e+01 -5.700e+01 -1.680e+02]\n",
            " [-8.910e+02 -1.026e+03 -9.100e+02 -9.170e+02 -8.440e+02 -5.140e+02\n",
            "  -1.030e+03 -8.680e+02 -8.030e+02 -9.180e+02]\n",
            " [-1.639e+03 -1.508e+03 -1.635e+03 -1.692e+03 -2.120e+03 -1.821e+03\n",
            "  -1.746e+03 -1.546e+03 -1.806e+03 -1.448e+03]\n",
            " [-1.708e+03 -2.380e+03 -1.691e+03 -1.320e+03 -1.808e+03 -1.682e+03\n",
            "  -1.757e+03 -1.602e+03 -1.699e+03 -1.571e+03]\n",
            " [-1.084e+03 -4.870e+02 -9.800e+02 -9.140e+02 -1.275e+03 -6.720e+02\n",
            "  -1.110e+03 -8.810e+02 -1.076e+03 -1.022e+03]\n",
            " [-3.240e+02 -2.860e+02 -2.640e+02 -2.060e+02 -3.290e+02  1.560e+02\n",
            "  -1.300e+02 -1.300e+01 -4.330e+02 -4.600e+01]\n",
            " [-3.200e+01 -2.000e+01 -1.400e+01 -3.500e+01  9.000e+01 -4.100e+01\n",
            "  -1.500e+01  2.200e+01 -5.700e+01 -6.500e+01]\n",
            " [ 0.000e+00 -2.000e+00 -1.400e+01 -1.000e+01  0.000e+00  4.000e+00\n",
            "   0.000e+00  0.000e+00  1.600e+01  0.000e+00]\n",
            " [-3.260e+02 -5.720e+02 -7.400e+01 -2.160e+02 -2.260e+02 -3.830e+02\n",
            "  -4.530e+02 -2.010e+02 -2.530e+02 -2.350e+02]\n",
            " [-1.814e+03 -2.167e+03 -1.755e+03 -1.550e+03 -1.947e+03 -1.624e+03\n",
            "  -1.901e+03 -1.580e+03 -1.706e+03 -1.677e+03]\n",
            " [-1.701e+03 -2.013e+03 -2.052e+03 -1.751e+03 -2.016e+03 -1.856e+03\n",
            "  -1.815e+03 -1.698e+03 -1.756e+03 -1.846e+03]\n",
            " [-1.730e+03 -1.622e+03 -1.514e+03 -1.547e+03 -2.076e+03 -1.708e+03\n",
            "  -2.005e+03 -1.344e+03 -1.999e+03 -1.832e+03]\n",
            " [-1.363e+03 -1.579e+03 -1.445e+03 -1.330e+03 -1.537e+03 -1.443e+03\n",
            "  -1.377e+03 -1.488e+03 -1.364e+03 -1.429e+03]\n",
            " [-3.790e+02 -5.870e+02 -3.240e+02 -1.170e+02 -4.170e+02 -2.910e+02\n",
            "  -2.630e+02 -3.900e+02 -1.700e+02 -2.430e+02]\n",
            " [-3.000e+01 -1.700e+01 -1.300e+01 -3.300e+01  7.600e+01 -2.300e+01\n",
            "  -2.000e+00  3.100e+01 -1.400e+01 -6.700e+01]\n",
            " [ 0.000e+00  0.000e+00 -6.000e+00 -6.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  9.000e+00  0.000e+00]\n",
            " [-3.920e+02 -2.600e+02 -5.250e+02 -4.570e+02 -3.720e+02 -5.330e+02\n",
            "  -5.500e+02 -7.370e+02 -2.910e+02 -4.030e+02]\n",
            " [-1.667e+03 -1.776e+03 -1.808e+03 -2.084e+03 -1.628e+03 -1.650e+03\n",
            "  -1.597e+03 -1.981e+03 -1.619e+03 -1.717e+03]\n",
            " [-1.472e+03 -1.055e+03 -1.710e+03 -1.825e+03 -1.350e+03 -1.436e+03\n",
            "  -1.419e+03 -1.662e+03 -1.502e+03 -1.414e+03]\n",
            " [-1.767e+03 -1.220e+03 -1.275e+03 -1.509e+03 -1.570e+03 -1.960e+03\n",
            "  -1.762e+03 -1.485e+03 -1.578e+03 -1.380e+03]\n",
            " [-1.162e+03 -1.490e+03 -1.378e+03 -1.557e+03 -1.238e+03 -1.593e+03\n",
            "  -1.603e+03 -1.277e+03 -1.235e+03 -8.810e+02]\n",
            " [-2.050e+02 -9.200e+01  4.800e+01 -2.050e+02 -2.150e+02 -7.430e+02\n",
            "  -3.040e+02 -1.620e+02 -2.320e+02 -2.320e+02]\n",
            " [-3.000e+00 -7.000e+00  0.000e+00 -1.000e+01  5.400e+01 -1.100e+01\n",
            "  -2.000e+00 -3.000e+00 -3.000e+00 -3.200e+01]\n",
            " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
            " [-2.000e+02 -4.820e+02 -4.870e+02 -5.430e+02 -3.600e+02 -2.070e+02\n",
            "  -8.400e+01 -4.520e+02 -2.310e+02 -3.790e+02]\n",
            " [-1.400e+03 -1.458e+03 -1.863e+03 -1.784e+03 -1.188e+03 -1.210e+03\n",
            "  -1.377e+03 -1.466e+03 -1.599e+03 -1.274e+03]\n",
            " [-1.791e+03 -1.758e+03 -2.103e+03 -1.986e+03 -1.610e+03 -1.731e+03\n",
            "  -1.661e+03 -2.007e+03 -1.541e+03 -1.465e+03]\n",
            " [-1.964e+03 -1.502e+03 -1.858e+03 -1.464e+03 -1.587e+03 -1.481e+03\n",
            "  -1.791e+03 -1.563e+03 -1.703e+03 -1.701e+03]\n",
            " [-1.125e+03 -9.130e+02 -1.156e+03 -1.360e+03 -1.295e+03 -1.167e+03\n",
            "  -1.096e+03 -1.076e+03 -1.060e+03 -8.820e+02]\n",
            " [-1.400e+02 -2.690e+02 -1.390e+02 -4.840e+02  1.500e+01 -4.620e+02\n",
            "  -3.720e+02  5.200e+01 -1.740e+02 -1.520e+02]\n",
            " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  6.000e+00 -1.000e+00\n",
            "  -1.000e+00 -3.000e+00 -1.000e+00 -3.000e+00]\n",
            " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
            " [-4.200e+01 -1.980e+02 -2.840e+02 -1.810e+02 -2.000e+00 -9.000e+01\n",
            "   5.500e+01 -1.000e+02 -3.390e+02 -5.400e+02]\n",
            " [-8.840e+02 -7.470e+02 -1.178e+03 -1.057e+03 -8.180e+02 -7.970e+02\n",
            "  -9.230e+02 -8.240e+02 -1.041e+03 -9.630e+02]\n",
            " [-1.642e+03 -1.586e+03 -1.592e+03 -1.442e+03 -1.566e+03 -1.675e+03\n",
            "  -1.302e+03 -1.551e+03 -1.168e+03 -1.329e+03]\n",
            " [-1.802e+03 -1.561e+03 -1.555e+03 -1.479e+03 -1.415e+03 -1.828e+03\n",
            "  -1.591e+03 -1.472e+03 -1.560e+03 -1.678e+03]\n",
            " [-1.242e+03 -1.125e+03 -1.365e+03 -1.396e+03 -1.007e+03 -1.196e+03\n",
            "  -1.348e+03 -1.021e+03 -1.465e+03 -1.238e+03]\n",
            " [-2.370e+02 -4.570e+02 -4.360e+02 -9.900e+01  1.150e+02 -1.240e+02\n",
            "   3.000e+00 -4.900e+01 -6.200e+02 -4.190e+02]\n",
            " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
            " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
            " [-2.010e+02 -3.450e+02 -1.500e+01 -1.950e+02  3.400e+02 -2.430e+02\n",
            "  -3.420e+02 -1.260e+02 -1.770e+02 -1.950e+02]\n",
            " [-7.450e+02 -1.026e+03 -9.230e+02 -1.136e+03 -8.820e+02 -1.260e+03\n",
            "  -5.680e+02 -9.500e+02 -6.200e+02 -1.308e+03]\n",
            " [-1.026e+03 -7.390e+02 -7.410e+02 -1.500e+03 -6.530e+02 -1.184e+03\n",
            "  -9.530e+02 -1.076e+03 -1.052e+03 -1.668e+03]\n",
            " [-1.292e+03 -1.319e+03 -1.665e+03 -1.073e+03 -9.720e+02 -1.316e+03\n",
            "  -1.066e+03 -1.215e+03 -9.700e+02 -1.651e+03]\n",
            " [-1.173e+03 -1.403e+03 -1.532e+03 -1.055e+03 -1.107e+03 -1.350e+03\n",
            "  -1.218e+03 -1.403e+03 -1.163e+03 -1.586e+03]\n",
            " [-3.760e+02 -7.380e+02 -4.770e+02 -8.200e+01 -1.330e+02 -3.260e+02\n",
            "  -2.220e+02 -3.360e+02 -1.790e+02 -6.980e+02]\n",
            " [ 0.000e+00 -2.000e+00  4.000e+00 -4.000e+00  0.000e+00  0.000e+00\n",
            "   4.000e+00  0.000e+00 -2.000e+00  0.000e+00]\n",
            " [ 0.000e+00 -3.000e+00  3.000e+00  0.000e+00 -3.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00 -3.000e+00  0.000e+00]\n",
            " [-2.120e+02 -4.700e+01  4.100e+01 -1.680e+02  8.000e+01  2.800e+01\n",
            "  -2.080e+02 -1.500e+02 -1.570e+02 -4.600e+01]\n",
            " [-1.014e+03 -1.265e+03 -1.151e+03 -1.221e+03 -1.445e+03 -1.296e+03\n",
            "  -1.040e+03 -1.247e+03 -9.860e+02 -1.299e+03]\n",
            " [-1.400e+03 -1.290e+03 -9.770e+02 -1.626e+03 -1.403e+03 -1.617e+03\n",
            "  -1.399e+03 -1.375e+03 -1.624e+03 -1.565e+03]\n",
            " [-1.388e+03 -1.096e+03 -1.073e+03 -1.411e+03 -1.406e+03 -1.345e+03\n",
            "  -1.518e+03 -1.548e+03 -1.701e+03 -1.422e+03]\n",
            " [-1.303e+03 -1.382e+03 -1.169e+03 -1.215e+03 -1.674e+03 -1.304e+03\n",
            "  -1.208e+03 -1.725e+03 -1.334e+03 -1.343e+03]\n",
            " [-4.420e+02 -6.320e+02 -2.890e+02 -1.060e+02 -5.860e+02 -6.710e+02\n",
            "  -3.170e+02 -4.880e+02 -2.370e+02 -3.380e+02]\n",
            " [-2.200e+01  9.700e+01  6.000e+00 -1.120e+02 -9.000e+00 -1.500e+01\n",
            "  -8.900e+01 -4.000e+00 -3.000e+01  6.100e+01]\n",
            " [ 0.000e+00 -1.000e+00  1.000e+00  0.000e+00 -1.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00 -1.000e+00  0.000e+00]\n",
            " [-1.700e+01 -1.000e+00  1.040e+02 -2.100e+01 -8.100e+01  3.400e+01\n",
            "  -4.000e+01 -1.000e+01 -5.400e+01 -1.090e+02]\n",
            " [-8.820e+02 -1.098e+03 -7.280e+02 -6.210e+02 -8.140e+02 -5.360e+02\n",
            "  -9.310e+02 -9.080e+02 -1.208e+03 -8.420e+02]\n",
            " [-1.500e+03 -1.583e+03 -1.605e+03 -1.599e+03 -1.862e+03 -1.452e+03\n",
            "  -1.812e+03 -1.869e+03 -1.719e+03 -1.554e+03]\n",
            " [-1.746e+03 -1.757e+03 -1.612e+03 -1.752e+03 -1.726e+03 -1.664e+03\n",
            "  -1.716e+03 -1.791e+03 -1.471e+03 -1.658e+03]\n",
            " [-1.090e+03 -1.064e+03 -9.650e+02 -1.074e+03 -1.321e+03 -9.980e+02\n",
            "  -1.021e+03 -1.303e+03 -1.270e+03 -1.041e+03]\n",
            " [-1.950e+02 -4.300e+01  8.200e+01 -1.580e+02 -3.160e+02 -4.460e+02\n",
            "  -2.200e+02 -3.670e+02 -4.130e+02 -1.700e+02]\n",
            " [-4.200e+01  1.600e+02  1.090e+02 -1.230e+02 -9.000e+00 -6.000e+01\n",
            "  -1.320e+02 -1.000e+01 -1.440e+02 -3.500e+01]]\n"
          ]
        }
      ],
      "source": [
        "W, E, k = perceptron(X_train, y_train, K=100);\n",
        "print(\"Número de iteraciones ejecutadas: \", k);\n",
        "print(\"Número de errores de entrenamiento durante la última iteración: \", E);\n",
        "print(\"Vectores de pesos de las clases (por columnas, notación homogénea):\\n\", W);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNKS9pzh3Ucg"
      },
      "source": [
        "### 5. Cálculo de tasas de error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Y_R4zp3Ucg"
      },
      "source": [
        "**Tasa de error en train:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2GwIdXz3Ucg",
        "outputId": "878d996b-3842-4b99-96b9-6955df4e4403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasa de error en train: 1.6%\n"
          ]
        }
      ],
      "source": [
        "# Matriz de muestras de train en notación homogénea:\n",
        "X_train_h = np.hstack([np.ones((len(X_train), 1)), X_train]);\n",
        "# Clasificamos muestras de test, obtenemos etiquetas:\n",
        "y_train_pred  = np.argmax(X_train_h @ W, axis=1).reshape(-1, 1);\n",
        "#print(np.hstack([y_train_pred,y_train]))   imprime la comparación entre el predicho y el original\n",
        "\n",
        "\n",
        "# Calculamos tasa error en train:\n",
        "err_train = np.count_nonzero(y_train_pred != y_train) / len(X_train);\n",
        "print(f\"Tasa de error en train: {err_train:.1%}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkLyfsVh3Ucg"
      },
      "source": [
        "**Tasa de error en test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJBBFQAd3Ucg",
        "outputId": "65371e5c-957d-4cad-b3e2-69571bc14877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasa de error en test: 5.3%\n"
          ]
        }
      ],
      "source": [
        "X_test_h = np.hstack([np.ones((len(X_test), 1)), X_test]);\n",
        "y_test_pred  = np.argmax(X_test_h @ W, axis=1).reshape(-1, 1); #devuelve un array de a que clase pertenece cada objeto segun nuestro perceptrón\n",
        "\n",
        "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test);\n",
        "print(f\"Tasa de error en test: {err_test:.1%}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_VCgtmL3Uch"
      },
      "source": [
        "**Interpretación de resultados:** $\\;$ los datos de entrenamiento no parecen linealmente separables (el algoritmo no converge). Con margen $b=0.1$ se obtiene un error de clasificación en train de $17.5%$ y en test del $26.7\\%$. Esto indica que el clasificador evaluado no generaliza bien (funciona significativamente peor con muestras no vistas en entrenamiento). Cabe matizar que se dispone de muy pocas muestras de test (30), por lo que los resultados pueden ser poco representativos o fiables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YL5ne9A3Uch"
      },
      "source": [
        "### 6. Optimización de hiperparámetros\n",
        "\n",
        "**Ajuste del margen:** $\\;$ experimento para optimizar el valor del hiperparámetro $b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXRwojQ03Uch",
        "outputId": "7beb7504-447a-47ec-cee0-d8cb329c8f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#        b\t E\t   k\t  ERROR TRAIN\t   ERROR TEST\n",
            "      0.00\t 9\t 100\t         0.10\t         0.17 \n",
            "      0.01\t 9\t 100\t         0.10\t         0.17 \n",
            "      0.10\t11\t 100\t         0.17\t         0.27 \n",
            "     10.00\t 8\t 100\t         0.09\t         0.17 \n",
            "    100.00\t17\t 100\t         0.06\t         0.07 \n",
            "   1000.00\t45\t 100\t         0.03\t         0.00 \n",
            "   1300.00\t50\t 100\t         0.04\t         0.00 \n"
          ]
        }
      ],
      "source": [
        "print(\"# {:>8s}\\t{:>2s}\\t{:>4s}\\t{:>13s}\\t{:>13s}\".format(\"b\", \"E\", \"k\",\"ERROR TRAIN\",\"ERROR TEST\"));\n",
        "for b in (.0, .01, .1, 10, 100, 1000,1300):\n",
        "    W, E, k = perceptron(X_train, y_train, b=b, K=100)\n",
        "\n",
        "    X_train_h = np.hstack([np.ones((len(X_train), 1)), X_train]);\n",
        "    y_train_pred  = np.argmax(X_train_h @ W, axis=1).reshape(-1, 1);\n",
        "    err_train = np.count_nonzero(y_train_pred != y_train) / len(X_train);\n",
        "\n",
        "    X_test_h = np.hstack([np.ones((len(X_test), 1)), X_test]);\n",
        "    y_test_pred  = np.argmax(X_test_h @ W, axis=1).reshape(-1, 1);\n",
        "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test);\n",
        "\n",
        "    print(\"  {:8.2f}\\t{:>2d}\\t{:>4d}\\t{:13.2f}\\t{:13.2f} \".format(b, E, k,err_train,err_test));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8adae5-7f2f-4d03-a5dc-a87d79376f1d",
        "id": "28Lccz4CRChO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#        b\t a\tk_aux\t E\t   k\t  ERROR TRAIN\t   ERROR TEST\n",
            "      0.00\t0.85\t100\t 0\t  70\t       0.0000\t       0.0417 \n",
            "      0.00\t0.85\t200\t 0\t  70\t       0.0000\t       0.0417 \n",
            "      0.00\t0.85\t300\t 0\t  70\t       0.0000\t       0.0417 \n",
            "      0.00\t0.90\t100\t 0\t  80\t       0.0000\t       0.0472 \n",
            "      0.00\t0.90\t200\t 0\t  80\t       0.0000\t       0.0472 \n",
            "      0.00\t0.90\t300\t 0\t  80\t       0.0000\t       0.0472 \n",
            "      0.00\t0.95\t100\t 0\t  85\t       0.0000\t       0.0500 \n",
            "      0.00\t0.95\t200\t 0\t  85\t       0.0000\t       0.0500 \n",
            "      0.00\t0.95\t300\t 0\t  85\t       0.0000\t       0.0500 \n",
            "      0.00\t1.00\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.00\t1.00\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.00\t1.00\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t0.85\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.01\t0.85\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t0.85\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t0.90\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.01\t0.90\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t0.90\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t0.95\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.01\t0.95\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t0.95\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t1.00\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.01\t1.00\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.01\t1.00\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t0.85\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.10\t0.85\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t0.85\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t0.90\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.10\t0.90\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t0.90\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t0.95\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.10\t0.95\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t0.95\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t1.00\t100\t10\t 100\t       0.0160\t       0.0528 \n",
            "      0.10\t1.00\t200\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      0.10\t1.00\t300\t 0\t 106\t       0.0000\t       0.0444 \n",
            "      1.00\t0.85\t100\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.85\t200\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.85\t300\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.90\t100\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.90\t200\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.90\t300\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.95\t100\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.95\t200\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t0.95\t300\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t1.00\t100\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t1.00\t200\t 0\t  88\t       0.0000\t       0.0417 \n",
            "      1.00\t1.00\t300\t 0\t  88\t       0.0000\t       0.0417 \n",
            "     10.00\t0.85\t100\t13\t 100\t       0.0028\t       0.0361 \n",
            "     10.00\t0.85\t200\t 0\t 107\t       0.0000\t       0.0417 \n",
            "     10.00\t0.85\t300\t 0\t 107\t       0.0000\t       0.0417 \n",
            "     10.00\t0.90\t100\t13\t 100\t       0.0028\t       0.0361 \n",
            "     10.00\t0.90\t200\t 0\t 107\t       0.0000\t       0.0417 \n",
            "     10.00\t0.90\t300\t 0\t 107\t       0.0000\t       0.0417 \n",
            "     10.00\t0.95\t100\t 6\t 100\t       0.0042\t       0.0500 \n",
            "     10.00\t0.95\t200\t 0\t 140\t       0.0000\t       0.0500 \n",
            "     10.00\t0.95\t300\t 0\t 140\t       0.0000\t       0.0500 \n",
            "     10.00\t1.00\t100\t 6\t 100\t       0.0042\t       0.0500 \n",
            "     10.00\t1.00\t200\t 0\t 140\t       0.0000\t       0.0500 \n",
            "     10.00\t1.00\t300\t 0\t 140\t       0.0000\t       0.0500 \n",
            "    100.00\t0.85\t100\t 9\t 100\t       0.0056\t       0.0444 \n",
            "    100.00\t0.85\t200\t 0\t 109\t       0.0000\t       0.0417 \n",
            "    100.00\t0.85\t300\t 0\t 109\t       0.0000\t       0.0417 \n",
            "    100.00\t0.90\t100\t 0\t  92\t       0.0000\t       0.0500 \n",
            "    100.00\t0.90\t200\t 0\t  92\t       0.0000\t       0.0500 \n",
            "    100.00\t0.90\t300\t 0\t  92\t       0.0000\t       0.0500 \n",
            "    100.00\t0.95\t100\t 8\t 100\t       0.0104\t       0.0528 \n",
            "    100.00\t0.95\t200\t 0\t 124\t       0.0000\t       0.0444 \n",
            "    100.00\t0.95\t300\t 0\t 124\t       0.0000\t       0.0444 \n",
            "    100.00\t1.00\t100\t 0\t  97\t       0.0000\t       0.0417 \n",
            "    100.00\t1.00\t200\t 0\t  97\t       0.0000\t       0.0417 \n",
            "    100.00\t1.00\t300\t 0\t  97\t       0.0000\t       0.0417 \n",
            "   1000.00\t0.85\t100\t 9\t 100\t       0.0021\t       0.0444 \n",
            "   1000.00\t0.85\t200\t 0\t 179\t       0.0000\t       0.0444 \n",
            "   1000.00\t0.85\t300\t 0\t 179\t       0.0000\t       0.0444 \n",
            "   1000.00\t0.90\t100\t11\t 100\t       0.0118\t       0.0417 \n",
            "   1000.00\t0.90\t200\t 0\t 115\t       0.0000\t       0.0472 \n",
            "   1000.00\t0.90\t300\t 0\t 115\t       0.0000\t       0.0472 \n",
            "   1000.00\t0.95\t100\t17\t 100\t       0.0035\t       0.0417 \n",
            "   1000.00\t0.95\t200\t 0\t 131\t       0.0000\t       0.0528 \n",
            "   1000.00\t0.95\t300\t 0\t 131\t       0.0000\t       0.0528 \n",
            "   1000.00\t1.00\t100\t 7\t 100\t       0.0035\t       0.0417 \n",
            "   1000.00\t1.00\t200\t 0\t 133\t       0.0000\t       0.0417 \n",
            "   1000.00\t1.00\t300\t 0\t 133\t       0.0000\t       0.0417 \n",
            "   1300.00\t0.85\t100\t11\t 100\t       0.0028\t       0.0361 \n",
            "   1300.00\t0.85\t200\t 0\t 183\t       0.0000\t       0.0472 \n",
            "   1300.00\t0.85\t300\t 0\t 183\t       0.0000\t       0.0472 \n",
            "   1300.00\t0.90\t100\t23\t 100\t       0.0042\t       0.0389 \n",
            "   1300.00\t0.90\t200\t 0\t 160\t       0.0000\t       0.0389 \n",
            "   1300.00\t0.90\t300\t 0\t 160\t       0.0000\t       0.0389 \n",
            "   1300.00\t0.95\t100\t21\t 100\t       0.0021\t       0.0528 \n",
            "   1300.00\t0.95\t200\t 0\t 169\t       0.0000\t       0.0528 \n",
            "   1300.00\t0.95\t300\t 0\t 169\t       0.0000\t       0.0528 \n",
            "   1300.00\t1.00\t100\t14\t 100\t       0.0021\t       0.0389 \n",
            "   1300.00\t1.00\t200\t 0\t 166\t       0.0000\t       0.0500 \n",
            "   1300.00\t1.00\t300\t 0\t 166\t       0.0000\t       0.0500 \n",
            "\n",
            "el error de test mínimo ha sido de 3.611111% con un nos valores de BETA, ALPHA y K de : 10.000, 0.85 y  100\n",
            "el error de entrenamiento mínimo ha sido de 0.000000% con un nos valores de BETA, ALPHA y K de : 0.000, 0.85 y  100\n"
          ]
        }
      ],
      "source": [
        "err_test_minimo = [1,0,0,0]\n",
        "err_train_minimo = [1,0,0,0]\n",
        "\n",
        "\n",
        "print(\"# {:>8s}\\t{:>2s}\\t{:>2s}\\t{:>2s}\\t{:>4s}\\t{:>13s}\\t{:>13s}\".format(\"b\",\"a\",\"k_aux\", \"E\", \"k\",\"ERROR TRAIN\",\"ERROR TEST\"));\n",
        "for b in (.0, .01, .1, 1, 10, 100, 1000,1300):\n",
        "    for a in (0.85,0.90,0.95,1):\n",
        "      for k_aux in (100, 200, 300):\n",
        "        W, E, k = perceptron(X_train, y_train, b=b, a=a, K=k_aux)\n",
        "\n",
        "        X_train_h = np.hstack([np.ones((len(X_train), 1)), X_train]);\n",
        "        y_train_pred  = np.argmax(X_train_h @ W, axis=1).reshape(-1, 1);\n",
        "        err_train = np.count_nonzero(y_train_pred != y_train) / len(X_train);\n",
        "\n",
        "        if err_train < err_train_minimo[0]:\n",
        "          err_train_minimo = [err_train,b,a,k_aux]\n",
        "\n",
        "        X_test_h = np.hstack([np.ones((len(X_test), 1)), X_test]);\n",
        "        y_test_pred  = np.argmax(X_test_h @ W, axis=1).reshape(-1, 1);\n",
        "        err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test);\n",
        "\n",
        "        if err_test < err_test_minimo[0]:\n",
        "          err_test_minimo = [err_test,b,a,k_aux]\n",
        "\n",
        "        print(\"  {:8.2f}\\t{:>2.2f}\\t{:>2d}\\t{:>2d}\\t{:>4d}\\t{:13.4f}\\t{:13.4f} \".format(b,a,k_aux, E, k,err_train,err_test));\n",
        "\n",
        "print(\"\\nel error de test mínimo ha sido de {:5%} con un nos valores de BETA, ALPHA y K de : {:.3f}, {:.2f} y {:4d}\".format(err_test_minimo[0],err_test_minimo[1],err_test_minimo[2],err_test_minimo[3]))\n",
        "print(\"el error de entrenamiento mínimo ha sido de {:5%} con un nos valores de BETA, ALPHA y K de : {:.3f}, {:.2f} y {:4d}\".format(err_train_minimo[0],err_train_minimo[1],err_train_minimo[2],err_train_minimo[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpAkNZdl3Uch"
      },
      "source": [
        "Viendo esta tabla, con el número de errores en entrenamiento acaecidos durante la última iteración, no podemos extraer conclusiones. Deberíamos evaluar la tasa de error en test (y quizás también en train) para determinar el valor de `b` óptimo (que permite generalizar mejor).\n",
        "\n",
        "Como regla general, **seleccionaremos el conjunto de valores de hiperparámetros que minimicen la tasa de error en test**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_SpQoNr3Uch"
      },
      "source": [
        "### 7. Ejercicios\n",
        "\n",
        "1. Modifica la celda de código anterior para que calcule las tasas de error en train y test para cada experimento y las muestre en la tabla de resultados, añadiendo sendas columnas adicionales.\n",
        "2. A partir de la información proporcionada por las tasas de error en train y test, determina cuál es el valor de `b` óptimo. Puedes ampliar la exploración con otros valores de `b`, para tratar de optimizar aún más la tasa de error.\n",
        "3. Explora también diferentes valores para los hiperparámetros `a` y `K`. En esta tarea tan pequeña y sencilla no tienen un gran impacto, pero su exploración en otras tareas puede proporcionar mejoras adicionales, especialmente con `K` cuando el algoritmo no converge (¿cuándo es mejor parar el entrenamiento?).  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sin-2324",
      "language": "python",
      "name": "sin-2324"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}