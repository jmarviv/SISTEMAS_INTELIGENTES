{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL4MBJLJG0hQ"
      },
      "source": [
        "# Regresión logística aplicada al corpus y tarea Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSRmTCXsG0hS"
      },
      "source": [
        "## 1. Lectura del corpus y partición:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rl9CP8P_G0hU",
        "outputId": "9c12f284-9f8c-4754-bf91-c5b5cb7b7392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABOsAAACTCAYAAADfop37AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHqNJREFUeJzt3XtUVWX+x/HPQQzBC6BdlDTwklloYlaKo4JFTt6xUbNyhLJg1owaloXVqEhj6XImweWycuWIWXRRJyhnrHQS7aKNobhWVoOVtFIrSzner7h/f/iDEfHCfoLDPuz3ay3+2fA5z3P295y99/myz94ey7IsAQAAAAAAAKhzAXU9AQAAAAAAAABn0KwDAAAAAAAAHIJmHQAAAAAAAOAQNOsAAAAAAAAAh6BZBwAAAAAAADgEzToAAAAAAADAIWjWAQAAAAAAAA5Bsw4AAAAAAABwCJp1AAAAAAAAgEPQrAMAAAAAAAAcwqfNupycHHk8Hnk8Hn300UdVfm9Zltq0aSOPx6PBgwfXyhx2796tjIwMFRUV1crjX8jx48eVnp6uiIgIBQcHq0ePHlq9erVP51BX3Fr3Q4cOafr06brzzjvVvHlzeTwe5eTk+Gz8uubWum/atEnjx49XdHS0GjdurGuuuUajRo1ScXGxz+ZQ19xa+23btmnkyJFq166dQkJCdPnll6tv37565513fDaHuuTWup9r5syZ8ng86ty5c53NwZfcWveCgoKK533uz8aNG302j7ri1rqX27x5s4YOHarmzZsrJCREnTt31rx583w+D19za92Tk5Mv+H73eDzatWuXz+ZSF9xad0navn27Ro8erdatWyskJESdOnVSZmamjhw54tN51AU3172wsFB33nmnmjVrpqZNm6p///51sq+pkzPrGjVqpNzc3CrL161bp507dyooKKjWxt69e7dmzJjh85WdnJys5557Tvfdd5+ys7PVoEEDDRw48Lwv/PrKbXX/5ZdflJmZqS+//FJdu3b12bhO47a6z549WytWrNDtt9+u7OxspaSkaP369brpppv0+eef+2weTuC22n/33Xc6ePCgkpKSlJ2dralTp0qShg4dqoULF/psHnXNbXU/286dO/XMM8+ocePGdTJ+XXJr3SdOnKilS5dW+unQoYPP51FX3Fj3999/X7GxsdqzZ4+mTp2q7OxsDR48WDt37vTpPOqS2+qemppa5X3+8ssvKyQkRDfccIOuvvpqn82lLrmt7t9//71uvfVWbdy4UePHj1dWVpZiY2M1ffp03XPPPT6bR11zW903b96s3r1769tvv9X06dM1bdo0bd++XXFxcfrvf//rs3lIUqBPR/t/AwcO1LJlyzRv3jwFBv5vCrm5uerevbt++eWXupjWeR05ckQhISG/6jH+85//6PXXX9ecOXM0efJkSdLYsWPVuXNnPf744/rkk09qYqqO57a6t2rVSj/88INatmypzz77TLfccksNzc6/uK3ujzzyiHJzc3XZZZdVLLv77rvVpUsXzZo1S6+88sqvnabfcFvtBw4cqIEDB1ZaNn78eHXv3l3PPfecUlJSftXj+wu31f1skydPVs+ePVVWVuao5+kLbq17nz59NGLEiBp5LH/ktrofOHBAY8eO1aBBg7R8+XIFBLjzikJuq3tsbKxiY2MrLfvoo4905MgR3Xfffb/qsf2J2+q+dOlSeb1effTRR4qOjpYkpaSk6PTp03r55ZdVWlqq8PDwmpiuo7mt7lOnTlVwcLA2bNigFi1aSJLGjBmjjh076sknn9SKFStqYqrVUid7mHvuuUd79+6t9DXQEydOaPny5br33nvPmzl8+LAeffRRtWnTRkFBQbruuuv017/+VZZlVfq71atXq3fv3goLC1OTJk103XXX6cknn5R05isL5Q2T+++/v+K0zvKvJsbHx6tz584qLCxU3759FRISUpHNz8/XoEGDFBERoaCgILVv315PP/20ysrKLvl8ly9frgYNGlT6oNaoUSONGzdOGzZs0Pfff1/9lefH3Fb3oKAgtWzZ0vZ6qm/cVvdevXpVatRJ0rXXXqvo6Gh9+eWX1Vtp9YTban8+DRo0UJs2beT1eo3y/sitdV+/fr2WL1+urKysamfqE7fWXZIOHjyoU6dO2crUF26re25urn766SfNnDlTAQEBOnz4sE6fPm17vfk7t9X9fHJzc+XxeC74fOsjt9X9wIEDkqSrrrqq0vJWrVopICCgyvF+feW2un/44YdKSEioaNRJZ2oeFxenlStX6tChQ9Vfeb9SnZxZFxUVpdjYWL322msaMGCAJGnVqlXav3+/Ro8eXeWaD5ZlaejQoVq7dq3GjRunmJgYvffee3rssce0a9cuzZ07V9KZ6wUNHjxYN954ozIzMxUUFKSvv/5aH3/8sSTp+uuvV2ZmpqZNm6aUlBT16dNH0pkP1+X27t2rAQMGaPTo0RozZkzFmzMnJ0dNmjTRI488oiZNmuiDDz7QtGnTdODAAc2ZM+eiz3fLli3q2LGjmjVrVmn5rbfeKkkqKipSmzZtTFen33Bb3XEGdT/znH766aeK/8q5hVtrf/jwYR09elT79+/X22+/rVWrVunuu+/+dSvTj7ix7mVlZZowYYIefPBBdenS5devRD/kxrpLZz5AHDp0SA0aNFCfPn00Z84c3Xzzzb9uZfoRt9V9zZo1atasmXbt2qXExEQVFxercePG+v3vf6+5c+eqUaNGNbNiHc5tdT/XyZMn9eabb6pXr16KiooyWof+yG11j4+P1+zZszVu3DjNmDFDLVq00CeffKLnn39eEydOdM0lL9xW9+PHjys4OLjK8pCQEJ04cUKff/65evbsabg2bbJ8aPHixZYka9OmTdb8+fOtpk2bWkeOHLEsy7JGjhxp9evXz7Isy4qMjLQGDRpUkcvLy7MkWX/5y18qPd6IESMsj8djff3115ZlWdbcuXMtSdbPP/98wTls2rTJkmQtXry4yu/i4uIsSdYLL7xQ5Xfl8zxbamqqFRISYh07duyizzs6Otq67bbbqizftm3bBcerT9xa9+qOX19R9/9ZunSpJclatGiR7aw/cnvtU1NTLUmWJCsgIMAaMWKEtW/fvmpl/Zmb6z5//nwrNDTU2rNnT8VY0dHRl8zVB26t+8cff2z97ne/sxYtWmTl5+dbzz77rNWiRQurUaNG1ubNmy+arQ/cWvcbb7zRCgkJsUJCQqwJEyZYK1assCZMmGBJskaPHn3RbH3g1rqf65133rEkWQsWLLCV81durvvTTz9tBQcHVxzXSbKeeuqpS+bqA7fWvUuXLlbHjh2tU6dOVSw7fvy4dc0111iSrOXLl180X5Pq7EILo0aN0tGjR7Vy5UodPHhQK1euvOBplP/617/UoEEDTZw4sdLyRx99VJZladWqVZKksLAwSWdOezQ9JT0oKEj3339/leVnd1cPHjyoX375RX369NGRI0f01VdfXfQxjx49et4LL5b/9+3o0aNGc/VHbqo7/sfNdf/qq6/0pz/9SbGxsUpKSjKapz9zY+3T0tK0evVqLVmyRAMGDFBZWZlOnDhhNE9/5aa67927V9OmTdPUqVN1xRVXGM2rvnBT3Xv16qXly5frgQce0NChQzVlyhRt3LhRHo9HTzzxhNE8/ZWb6n7o0CEdOXJEY8eO1bx583TXXXdp3rx5Sk1N1euvv67t27cbzdUfuanu58rNzVXDhg01atQoozn6M7fVPSoqSn379tXChQu1YsUKPfDAA3rmmWc0f/58o3n6KzfV/Y9//KOKi4s1btw4ffHFF/r88881duxY/fDDD5J827ups2bdFVdcoYSEBOXm5uof//iHysrKLniB3u+++04RERFq2rRppeXXX399xe+lMxdx/81vfqMHH3xQV111lUaPHq0333zTVvGvvvrq837/fNu2bRo+fLhCQ0PVrFkzXXHFFRozZowkaf/+/Rd9zODgYB0/frzK8mPHjlX83i3cVHf8j1vr/uOPP2rQoEEKDQ2tuHal27ix9p06dVJCQoLGjh1bcW2LIUOGVLlOR33mprr/+c9/VvPmzTVhwoRqz6O+clPdz6dDhw4aNmyY1q5da3wdLH/kprqXH7OfeyfI8g+tGzZsqPb8/J2b6n62Q4cOKT8/X7/97W8rXdPKLdxU99dff10pKSl66aWX9NBDD+muu+7SokWLlJSUpPT0dO3du7fa8/N3bqr7H/7wBz355JPKzc1VdHS0unTpom+++UaPP/64JKlJkybVnt+vVSfXrCt377336qGHHtKPP/6oAQMGVHRXTQUHB2v9+vVau3at/vnPf+rdd9/VG2+8odtuu03vv/9+tT4on69x5vV6FRcXp2bNmikzM1Pt27dXo0aNtHnzZqWnp1/yBdWqVSvt2rWryvLy7mxEREQ1n2H94Ja6ozK31X3//v0aMGCAvF6vPvzwQ9e9z8/mttqfa8SIEUpNTVVxcbGuu+46o8fwR26o+/bt27Vw4UJlZWVp9+7dFcuPHTumkydPqqSkRM2aNVPz5s3NnrQfckPdL6ZNmzY6ceKEDh8+XOVaxfWZW+oeERGhbdu2Vbng/JVXXilJKi0ttfEs/Z9b6n62vLw8190F9lxuqfuCBQvUrVs3tW7dutLyoUOHKicnR1u2bFFCQoK9J+vH3FJ3SZo5c6YmT56sbdu2KTQ0VF26dKm4eUXHjh3tP1lDdXq/8eHDhysgIEAbN2686J10IiMjtXv3bh08eLDS8vJTGCMjIyuWBQQE6Pbbb9dzzz2nL774QjNnztQHH3ygtWvXSpI8Ho/teRYUFGjv3r3KycnRww8/rMGDByshIaHat2qOiYlRcXFxxR1lyn366acVv3cTt9Qdlbmp7seOHdOQIUNUXFyslStX6oYbbrA9j/rETbU/n/LT5d12Nq4b6r5r1y6dPn1aEydOVNu2bSt+Pv30UxUXF6tt27bKzMy0PSd/5oa6X8y3336rRo0a+fQ/707glrp3795dkqr8E768We+2r8K7pe5ne/XVV9WkSRMNHTrUdra+cEvdf/rpp/OeJX3y5ElJct1dwN1S93Lh4eHq3bt3xY3D1qxZo9atW6tTp06252SqTpt1TZo00fPPP6+MjAwNGTLkgn83cOBAlZWVVflu+Ny5c+XxeCruSrJv374q2fJGWPnXUMvv2uL1eqs9z/Ku7tlfYTpx4oQWLFhQrfyIESNUVlamhQsXViw7fvy4Fi9erB49erjiTrBnc0vdUZlb6l5WVqa7775bGzZs0LJlyxQbG1vtsesrt9R+z549VZadPHlSL7/8soKDg13XtHVD3Tt37qy33nqryk90dLSuueYavfXWWxo3bly151IfuKHukvTzzz9XWbZ161a9/fbb6t+/vwIC6vQQ2+fcUvfya5QtWrSo0vKXXnpJgYGBio+Pr/Zc6gO31L3czz//rDVr1mj48OEKCQmxla1P3FL3jh07asuWLSouLq60/LXXXlNAQIBuvPHGas+lPnBL3c/njTfe0KZNm5SWlubT/Xudfg1WUrUuuD5kyBD169dPTz31lEpKStS1a1e9//77ys/PV1pamtq3by9JyszM1Pr16zVo0CBFRkZqz549WrBggVq3bq3evXtLktq3b6+wsDC98MILatq0qRo3bqwePXqobdu2Fxy/V69eCg8PV1JSkiZOnCiPx6OlS5dW+/pDPXr00MiRI/XEE09oz5496tChg5YsWaKSkpIqO3u3cEPdJWn+/Pnyer0V/3F95513tHPnTknShAkTFBoaWu3Hqg/cUPdHH31Ub7/9toYMGaJ9+/bplVdeqfT78usluI0bap+amqoDBw6ob9++uvrqq/Xjjz/q1Vdf1VdffaW//e1vrjvTRqr/db/88suVmJhYZXlWVpYknfd3blDf6y6dudZOcHCwevXqpSuvvFJffPGFFi5cqJCQEM2aNataj1HfuKHu3bp10wMPPKC///3vOnXqlOLi4lRQUKBly5bpiSeecOUlL9xQ93JvvPGGTp065eqvwJZzQ90fe+wxrVq1Sn369NH48ePVokULrVy5UqtWrdKDDz7I+/0C/L3u69evV2Zmpvr3768WLVpo48aNWrx4se688049/PDD1XqMGuODO85WOPv2vxdz7u1/LcuyDh48aE2aNMmKiIiwGjZsaF177bXWnDlzrNOnT1f8zb///W9r2LBhVkREhHXZZZdZERER1j333GMVFxdXeqz8/HzrhhtusAIDAyvdCjguLs6Kjo4+75w+/vhjq2fPnlZwcLAVERFhPf7449Z7771nSbLWrl17yed+9OhRa/LkyVbLli2toKAg65ZbbrHefffdS+bqAzfXPTIystKtvs/+2bFjxyXz/sytdS+/jfiFftzArbV/7bXXrISEBOuqq66yAgMDrfDwcCshIcHKz8+/aK6+cGvdz+diY9U3bq17dna2deutt1rNmze3AgMDrVatWlljxoyxtm/fftFcfeHWuluWZZ04ccLKyMiwIiMjrYYNG1odOnSw5s6de8lcfeDmuluWZfXs2dO68sorrVOnTlXr7+sLN9f9008/tQYMGGC1bNnSatiwodWxY0dr5syZ1smTJy+Z9XdurfvXX39t9e/f37r88sutoKAgq1OnTtazzz5rHT9+/KK52uCxLBfdng4AAAAAAABwMHddUAMAAAAAAABwMJp1AAAAAAAAgEPQrAMAAAAAAAAcgmYdAAAAAAAA4BA06wAAAAAAAACHoFkHAAAAAAAAOATNOgAAAAAAAMAhAut6AudatmyZUS49Pd125o477rCdmTVrlu1MeHi47QyqJz4+3nbG6/XazmRkZNjOJCYm2s6gegoKCmxnTOoRExNjO2MyN7eZPXu2UW7KlCm2M23btrWdKSwstJ1hO197TLbZycnJtjN5eXm2M6gek321JEVFRdnO5OTkGI0F5/DVsV1RUZHtDC4tKyvLKGdSQ5Pt9tatW21nQkNDbWckqaSkxHYmLCzMaKy6lpaWZpQzqaHJPt5kfv5aC18y/bxr8n5342cszqwDAAAAAAAAHIJmHQAAAAAAAOAQNOsAAAAAAAAAh6BZBwAAAAAAADgEzToAAAAAAADAIWjWAQAAAAAAAA5Bsw4AAAAAAABwCJp1AAAAAAAAgEPQrAMAAAAAAAAcgmYdAAAAAAAA4BA06wAAAAAAAACHCKzrCZwrPT3dKLdjxw7bmdLSUtuZ5s2b2868+eabtjOSNHLkSKOcm4SFhdnOrFu3znamoKDAdiYxMdF2xm2KioqMcv369bOdCQ0NtZ0pKSmxnXGbKVOm2M6YbhNffPFF25nU1FTbmcLCQtuZhIQE2xlUT05Oju1MTExMjc8D5ky3pSb76yVLltjOREZG2s6wf7i0vLw8o5xJ3adPn240FpzD5Jg+KyvLJxmv12s7I5k9J39lekxvwuS4wOSznEnGn5ns1/Lz82t+Ihfg8XhsZ7p27Wo748vX8qVwZh0AAAAAAADgEDTrAAAAAAAAAIegWQcAAAAAAAA4BM06AAAAAAAAwCFo1gEAAAAAAAAOQbMOAAAAAAAAcAiadQAAAAAAAIBD0KwDAAAAAAAAHIJmHQAAAAAAAOAQNOsAAAAAAAAAh6BZBwAAAAAAADgEzToAAAAAAADAIQJr88ELCwttZ3bs2GE01jfffGM7065dO9uZO+64w3bGZD1I0siRI41y/qioqMgoV1BQUKPzuJCYmBifjOM2eXl5RrmuXbvaziQmJtrOzJgxw3bGbVJSUmxn0tPTjcbq3r277Uzbtm1tZxISEmxncGler9col5OTYzuTlpZmO1NSUmI7YyoqKspnYzlBWFiYUe67776znQkNDbWdiY+Pt50xfT2brgt/lJGR4bOxTPbxqB0m219TJq8xk229rz5v+DPTz0om+0OT4wKTba9p3U32KU5gul8zERcXZztj8lrx9/cuZ9YBAAAAAAAADkGzDgAAAAAAAHAImnUAAAAAAACAQ9CsAwAAAAAAAByCZh0AAAAAAADgEDTrAAAAAAAAAIegWQcAAAAAAAA4BM06AAAAAAAAwCFo1gEAAAAAAAAOQbMOAAAAAAAAcAiadQAAAAAAAIBD0KwDAAAAAAAAHIJmHQAAAAAAAOAQgbX54KWlpbYzN910k9FY7dq1M8rZ1b17d5+M48+ysrJsZzIyMozG2r9/v1HOrvj4eJ+M4zZpaWlGuaioKJ+MNWzYMNsZtzHZ9n777bdGY+3YscN2JiEhwXbGZN8VHh5uO+M2OTk5RrmSkhLbmeTkZNsZk21EWFiY7Yxkvs/zVybbbEnaunWr7YzJcUFMTIztjGnt3cTr9RrlunbtajtjUkNcWkFBgU8ypkw+c5jIy8szypnsi/yV6XPt1q2b7YzJcYHJNtt03+WvfPl8Td5TiYmJtjOm+yGn4Mw6AAAAAAAAwCFo1gEAAAAAAAAOQbMOAAAAAAAAcAiadQAAAAAAAIBD0KwDAAAAAAAAHIJmHQAAAAAAAOAQNOsAAAAAAAAAh6BZBwAAAAAAADgEzToAAAAAAADAIWjWAQAAAAAAAA5Bsw4AAAAAAABwCJp1AAAAAAAAgEME1uaDl5aW2s7ccccdtTCTmmPynMLDw2thJs6VlpZmO5OcnGw0lq/Wrdfr9ck4/sxkHWVlZRmNlZeXZ5SzKycnxyfjuE27du2Mcvv27bOdSUhI8ElmzZo1tjOS/+4fTN6DkyZNMhorKSnJKGdXdna27czixYtrYSb1j+k2u6CgwHamqKjIdsb0tWnC5BjJX5keO0VFRdnOmBxPJCYm2s6YzM2fmTxfk/egZPZ+N2GyPYqPj6/xedQ3vvystG7dOtuZHTt22M647f0eFhZmO9O1a1ejsUyOfx9++GHbGZPtUUlJie2MVDuvF86sAwAAAAAAAByCZh0AAAAAAADgEDTrAAAAAAAAAIegWQcAAAAAAAA4BM06AAAAAAAAwCFo1gEAAAAAAAAOQbMOAAAAAAAAcAiadQAAAAAAAIBD0KwDAAAAAAAAHIJmHQAAAAAAAOAQNOsAAAAAAAAAh6BZBwAAAAAAADhEYG0+eHh4uO1MYWFhLczk/EpLS21nPvvsM9uZUaNG2c7AWYqKimxnYmJianweTpaRkWE7k52dXfMTuYC33nrLdiYsLKzmJwJjJvuUNWvW2M6kpqbazsyePdt2RpJmzZpllKtrJu+N0NBQo7GWLFliO2OyzTaRmJjok3HcKj4+vq6ncEElJSV1PQXHi4qKMsqtW7fOdsbr9drOTJo0yXZmy5YttjOS/x4TmtQwLy/PaCyPx2M7Y3Js5+TtilOY7EP79etnNNb06dNtZ0y2vyb7a9PXsum2zx+ZHm85+bN1WlqaUc709XIxnFkHAAAAAAAAOATNOgAAAAAAAMAhaNYBAAAAAAAADkGzDgAAAAAAAHAImnUAAAAAAACAQ9CsAwAAAAAAAByCZh0AAAAAAADgEDTrAAAAAAAAAIegWQcAAAAAAAA4BM06AAAAAAAAwCFo1gEAAAAAAAAOQbMOAAAAAAAAcIjA2nzwdu3a2c589tlnRmMtW7bMJxkT6enpPhkHqEvJycm2MwUFBUZjbd261XZm+PDhtjPDhg2znTFZD5KUmJholPNHU6ZMMcolJCTYzpSWltrOrF692nZm1KhRtjP+LD4+3nbG6/UajVVUVGQ7YzK/pKQk25mwsDDbGTfKy8szypms34yMDKOx7HLTNtuU6f5w0qRJtjNRUVG2MyUlJbYzpq/lmJgYo5w/SktLM8qFhobazphs63FpJu8nk/pJZq8Xk/dut27dbGdycnJsZyTf7Yf8mck20eS1YlJD0+18beDMOgAAAAAAAMAhaNYBAAAAAAAADkGzDgAAAAAAAHAImnUAAAAAAACAQ9CsAwAAAAAAAByCZh0AAAAAAADgEDTrAAAAAAAAAIegWQcAAAAAAAA4BM06AAAAAAAAwCFo1gEAAAAAAAAOQbMOAAAAAAAAcAiadQAAAAAAAIBD0KwDAAAAAAAAHCKwNh+8Xbt2tjOzZ882Gis9Pd125uabb7adKSwstJ3BpYWFhRnlhg0bZjuTn59vO1NQUGA7k5ycbDvjz2JiYmxnioqKjMYyyWVkZNjOmLxWoqKibGckKTEx0Sjnj8LDw41yKSkpNTyT8xs1apTtzIsvvlgLM4Fktn/Yv3+/7Yzbttm+ZLIPlaTs7OyancgFJCUl2c7Ex8fX/ETqGdP3VElJie1MTk6O7YxJDd20rzZl+n43qaHp5wdcnMl6Nd0mmhwThoaG2s6YfGZMS0uznXEb03Vk8lnO6/Xazphsj0w+09YWzqwDAAAAAAAAHIJmHQAAAAAAAOAQNOsAAAAAAAAAh6BZBwAAAAAAADgEzToAAAAAAADAIWjWAQAAAAAAAA5Bsw4AAAAAAABwCJp1AAAAAAAAgEPQrAMAAAAAAAAcgmYdAAAAAAAA4BA06wAAAAAAAACHoFkHAAAAAAAAOITHsiyrricBAAAAAAAAgDPrAAAAAAAAAMegWQcAAAAAAAA4BM06AAAAAAAAwCFo1gEAAAAAAAAOQbMOAAAAAAAAcAiadQAAAAAAAIBD0KwDAAAAAAAAHIJmHQAAAAAAAOAQNOsAAAAAAAAAh/g/bHI2u0ur7Q4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Lectura/carga del corpus\n",
        "digits = load_digits()\n",
        "X = digits.data.astype(np.float16) # muestras\n",
        "y = digits.target.astype(np.uint)  # etiquetas de clase\n",
        "\n",
        "# Partición (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=22)\n",
        "\n",
        "#mostrar numeros\n",
        "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(16, 16))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"none\")\n",
        "    ax.set_title(\"Mostra {!s}\".format(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxCjELtIG0hV"
      },
      "source": [
        "## 2. Regresión logística\n",
        "\n",
        "Usamos la implementación de regresión logística (clase [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) de la librería [sklearn](https://scikit-learn.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E4oxHwAnG0i4",
        "outputId": "8a6ccb50-d68e-4279-e466-7741328c902a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error de test: 2.8%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Código para filtrar los (molestos) warnings de sklearn sobre convergencia\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
        "\n",
        "# entrenamiento\n",
        "model = LogisticRegression(random_state=22).fit(X_train, y_train)\n",
        "\n",
        "# clasificación (test)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# cálculo tasa de error de test\n",
        "err_test = 1 - accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Error de test: {err_test:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZn5W25JG0i6"
      },
      "source": [
        "## 3. Ajuste de hiperparámetros\n",
        "\n",
        "A continuación se describen algunos hiperparámetros del [método constructor `LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) que pueden ser ajustados experimentalmente para minimzar la tasa de error en test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx73WAI-G0i7"
      },
      "source": [
        "### 3.1 Tolerancia\n",
        "\n",
        "Como sabemos, el algoritmo de descenso por gradiente actualiza en cada iteración los parámetros del modelo:\n",
        "\n",
        "$$\\boldsymbol{W}_{i+1}=\\boldsymbol{W}_i-\\eta_i\\boldsymbol{\\nabla}\\mathcal{L}(\\boldsymbol{W})\\rvert_{\\boldsymbol{W}_i}\\qquad i=0,1,\\ldots$$\n",
        "\n",
        "El parámetro `tol` establece el umbral de tolerancia mínimo necesario para continuar dicho entrenamiento. Si la magnitud de actualización de parámetros $\\Delta = \\lVert \\,  \\eta_i\\boldsymbol{\\nabla}\\mathcal{L}(\\boldsymbol{\\theta})\\rvert_{\\boldsymbol{\\theta}_i} \\, \\rVert$ es inferior a `tol` ($\\Delta < $ `tol`), el entrenamiento finaliza. Por defecto, `tol = 1e-4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Yn8O5wXdG0i8",
        "outputId": "d581603b-dcde-4280-c6cb-e62a6b8edc30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error de test con tolerancia 0.0001: 2.8%\n",
            "Error de test con tolerancia 0.01: 3.6%\n",
            "Error de test con tolerancia 1: 18.9%\n",
            "Error de test con tolerancia 100.0: 91.1%\n",
            "Error de test con tolerancia 10000.0: 91.1%\n"
          ]
        }
      ],
      "source": [
        "for tol in (1e-4, 1e-2, 1, 1e2, 1e4):\n",
        "    model = LogisticRegression(tol=tol, random_state=22).fit(X_train, y_train)\n",
        "    err_test = 1 - accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Error de test con tolerancia {tol}: {err_test:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp5MXUkZG0i9"
      },
      "source": [
        "### 3.2 Regularización\n",
        "\n",
        "La regularización en aprendizaje automático es una técnica utilizada para evitar el sobreajuste (overfitting) de los modelos. El sobreajuste ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento, pero falla en generalizar a nuevos datos. La regularización introduce una penalización o coste adicional en la función de riesgo empírico o pérdida $\\mathcal{L}(\\mathbf{\\theta})$, desincentivando la complejidad excesiva del modelo (p.e. pesos $w_k$ demasiado grandes) y forzándolo a enfocarse en patrones más generales.\n",
        "\n",
        "En regresión logística, reescribimos la función de riesgo empírico $\\mathcal{L}$ como la $\\operatorname{NLL}(\\mathbf{W})$, a la que se añade un término o función regularizadora $r(\\mathbf{W})$ escalada con un hiperparámetro de ajuste $C$:\n",
        "\n",
        "$$ \\mathcal{L}(\\mathbf{W}) = \\operatorname{NLL}(\\mathbf{W}) + \\frac{r(\\mathbf{W})}{C} $$\n",
        "\n",
        "`LogisticRegression` añade por defecto una regularización del criterio de entrenamiento basada en la norma $\\ell_2$:\n",
        "\n",
        "$$ r(\\mathbf{W}) = \\lambda \\, \\lVert \\mathbf{W} \\rVert_2^2$$\n",
        "\n",
        "con $\\lambda = \\frac{1}{2}$ (más detalles [aquí](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)). Entonces:\n",
        "\n",
        "$$ \\mathcal{L}(\\mathbf{W}) = \\operatorname{NLL}(\\mathbf{W}) +  \\frac{\\frac{1}{2} \\, \\lVert \\mathbf{W} \\rVert_2^2}{C} = \\operatorname{NLL}(\\mathbf{W}) + \\frac{\\sum_{i=0}^D \\sum_{j=1}^{K} \\mathbf{W}_{i,j}^2}{2C}$$\n",
        "\n",
        "El propósito de esta regularización es evitar un sobre-ajuste del modelo a los datos de entrenamiento, apostando por unos parámetros más sencillos (más próximos a cero):\n",
        "\n",
        "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}}\\;\\mathcal{L}(\\mathbf{W}) = \\operatorname{NLL}(\\mathbf{W}) + \\frac{\\sum_{i=0}^D \\sum_{j=0}^{K-1} \\mathbf{W}_{i,j}^2}{2C} $$\n",
        "\n",
        "El hiperparámetro $C$ (positivo, $1.0$ por defecto) permite ajustar a la inversa la magnitud de dicha regularización:\n",
        "* **Máxima regularización** (sub-ajuste a los datos de train): $\\;$ con un valor de `C` próximo a cero.\n",
        "* **Mínima regularización** (sobre-ajuste a los datos de train): $\\;$ con un valor de `C` positivo muy alto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "17Z0U7GEG0i9",
        "outputId": "986337db-2a8a-454b-f499-20fff67130b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error de test con C 0.001: 4.2%\n",
            "Error de test con C 0.01: 2.5%\n",
            "Error de test con C 0.1: 2.2%\n",
            "Error de test con C 1: 2.8%\n",
            "Error de test con C 10: 3.3%\n",
            "Error de test con C 100: 3.3%\n"
          ]
        }
      ],
      "source": [
        "for C in (1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
        "    model = LogisticRegression(C=C, random_state=22).fit(X_train, y_train)\n",
        "    err_test = 1 - accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Error de test con C {C:g}: {err_test:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrxp0dq_G0i-"
      },
      "source": [
        "### 3.3 Número de iteraciones máximas\n",
        "\n",
        "El parámetro `max_iter` ($100$ por defecto) permite ajustar el número total de iteraciones del algoritmo de optimización. Ajustaremos este número basándonos en el criterio *Early stopping*: detenemos el entrenamiento lo más pronto posible (en pocas iteraciones) para evitar un sobre-entrenamiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5VkyRaQ3G0i-",
        "outputId": "6325f51d-a585-4706-cd8f-68801f5c241d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error de test con max_iter 10: 3.6%\n",
            "Error de test con max_iter 20: 4.2%\n",
            "Error de test con max_iter 50: 3.1%\n",
            "Error de test con max_iter 100: 2.8%\n"
          ]
        }
      ],
      "source": [
        "for max_iter in (10, 20, 50, 100):\n",
        "    model = LogisticRegression(random_state=22, max_iter=max_iter).fit(X_train, y_train)\n",
        "    err_test = 1 - accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Error de test con max_iter {max_iter}: {err_test:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77--7w-LG0i_"
      },
      "source": [
        "## 4. Ejercicio\n",
        "\n",
        "Realiza una exploración combinada de diferentes hiperparámetros, calculando las tasas de error en train y test para cada experimento, mostrándolas en una tabla de resultados (similar a la del [cuaderno del Perceptrón](../P2.S2%20Perceptrón/01_iris.ipynb)). A partir de esta información, determina los valores óptimos de los hiperparámetros. Recuerda que, como regla general, **seleccionaremos el conjunto de valores de hiperparámeros que minimicen la tasa de error en test**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "err_test_minimo = [1,0,0,0]\n",
        "err_train_minimo = [1,0,0,0]\n",
        "\n",
        "\n",
        "print(\"# {:>6s}\\t{:>8s}\\t{:>4s}\\t{:>8s}\\t{:>13s}\".format(\"C\",\"max_iter\",\"tol\",\"ERROR TEST\",\"ERROR TRAIN\"));\n",
        "for C in (1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
        "  for max_iter in (10, 20, 50, 100):\n",
        "    for tol in (1e-4, 1e-2, 1, 1e2, 1e4):\n",
        "      model = LogisticRegression(C = C, tol = tol, max_iter=max_iter ,random_state=22).fit(X_train, y_train)\n",
        "\n",
        "\n",
        "      err_test = 1 - accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "      if err_test < err_test_minimo[0]:\n",
        "        err_test_minimo = [err_test,C,max_iter,tol]\n",
        "\n",
        "\n",
        "      err_train = 1 - accuracy_score(y_train, model.predict(X_train))\n",
        "\n",
        "      if err_train < err_train_minimo[0]:\n",
        "        err_train_minimo = [err_train,C,max_iter,tol]\n",
        "\n",
        "      print(\"{:8.3f}{:15.2f}{:13.4f}\\t{:10.5%}\\t{:13.5%} \".format(C,max_iter,tol,err_test, err_train));\n",
        "\n",
        "print(\"\\nel error de test mínimo ha sido de {:5%} con un nos valores de C, iteraciones máximas y tolerancia de : {:.3f}, {:.2f} y {:4f}\".format(err_test_minimo[0],err_test_minimo[1],err_test_minimo[2],err_test_minimo[3]))\n",
        "print(\"el error de entrenamiento mínimo ha sido de {:5%} con un nos valores de C, iteraciones máximas y tolerancia de : {:.3f}, {:.2f} y {:.4f}\".format(err_train_minimo[0],err_train_minimo[1],err_train_minimo[2],err_train_minimo[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWjGo9rfJVC8",
        "outputId": "753efc97-9cfe-4129-a744-130f1b8e756e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#      C\tmax_iter\t tol\tERROR TEST\t  ERROR TRAIN\n",
            "   0.001          10.00       0.0001\t  4.16667%\t     3.61865% \n",
            "   0.001          10.00       0.0100\t  4.16667%\t     3.61865% \n",
            "   0.001          10.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.001          10.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.001          10.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.001          20.00       0.0001\t  3.61111%\t     2.99235% \n",
            "   0.001          20.00       0.0100\t  3.61111%\t     2.99235% \n",
            "   0.001          20.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.001          20.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.001          20.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.001          50.00       0.0001\t  3.33333%\t     2.99235% \n",
            "   0.001          50.00       0.0100\t  3.61111%\t     2.92276% \n",
            "   0.001          50.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.001          50.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.001          50.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.001         100.00       0.0001\t  4.16667%\t     3.13152% \n",
            "   0.001         100.00       0.0100\t  3.61111%\t     2.92276% \n",
            "   0.001         100.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.001         100.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.001         100.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.010          10.00       0.0001\t  3.61111%\t     3.54906% \n",
            "   0.010          10.00       0.0100\t  3.61111%\t     3.54906% \n",
            "   0.010          10.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.010          10.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.010          10.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.010          20.00       0.0001\t  2.50000%\t     1.53097% \n",
            "   0.010          20.00       0.0100\t  2.50000%\t     1.53097% \n",
            "   0.010          20.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.010          20.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.010          20.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.010          50.00       0.0001\t  2.50000%\t     0.97425% \n",
            "   0.010          50.00       0.0100\t  2.50000%\t     1.11343% \n",
            "   0.010          50.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.010          50.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.010          50.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.010         100.00       0.0001\t  2.50000%\t     0.97425% \n",
            "   0.010         100.00       0.0100\t  2.50000%\t     1.11343% \n",
            "   0.010         100.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.010         100.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.010         100.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.100          10.00       0.0001\t  3.61111%\t     3.20111% \n",
            "   0.100          10.00       0.0100\t  3.61111%\t     3.20111% \n",
            "   0.100          10.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.100          10.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.100          10.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.100          20.00       0.0001\t  2.50000%\t     1.32220% \n",
            "   0.100          20.00       0.0100\t  2.50000%\t     1.32220% \n",
            "   0.100          20.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.100          20.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.100          20.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.100          50.00       0.0001\t  3.05556%\t     0.06959% \n",
            "   0.100          50.00       0.0100\t  2.77778%\t     0.27836% \n",
            "   0.100          50.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.100          50.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.100          50.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   0.100         100.00       0.0001\t  2.22222%\t     0.06959% \n",
            "   0.100         100.00       0.0100\t  2.77778%\t     0.27836% \n",
            "   0.100         100.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   0.100         100.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   0.100         100.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   1.000          10.00       0.0001\t  3.61111%\t     3.20111% \n",
            "   1.000          10.00       0.0100\t  3.61111%\t     3.20111% \n",
            "   1.000          10.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   1.000          10.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   1.000          10.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   1.000          20.00       0.0001\t  4.16667%\t     1.73974% \n",
            "   1.000          20.00       0.0100\t  4.16667%\t     1.73974% \n",
            "   1.000          20.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   1.000          20.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   1.000          20.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   1.000          50.00       0.0001\t  3.05556%\t     0.00000% \n",
            "   1.000          50.00       0.0100\t  3.61111%\t     0.13918% \n",
            "   1.000          50.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   1.000          50.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   1.000          50.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "   1.000         100.00       0.0001\t  2.77778%\t     0.00000% \n",
            "   1.000         100.00       0.0100\t  3.61111%\t     0.13918% \n",
            "   1.000         100.00       1.0000\t 18.88889%\t    16.77105% \n",
            "   1.000         100.00     100.0000\t 91.11111%\t    89.83994% \n",
            "   1.000         100.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "  10.000          10.00       0.0001\t  3.61111%\t     3.13152% \n",
            "  10.000          10.00       0.0100\t  3.61111%\t     3.13152% \n",
            "  10.000          10.00       1.0000\t 18.88889%\t    16.77105% \n",
            "  10.000          10.00     100.0000\t 91.11111%\t    89.83994% \n",
            "  10.000          10.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "  10.000          20.00       0.0001\t  4.16667%\t     1.67015% \n",
            "  10.000          20.00       0.0100\t  4.16667%\t     1.67015% \n",
            "  10.000          20.00       1.0000\t 18.88889%\t    16.77105% \n",
            "  10.000          20.00     100.0000\t 91.11111%\t    89.83994% \n",
            "  10.000          20.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "  10.000          50.00       0.0001\t  3.33333%\t     0.00000% \n",
            "  10.000          50.00       0.0100\t  3.61111%\t     0.13918% \n",
            "  10.000          50.00       1.0000\t 18.88889%\t    16.77105% \n",
            "  10.000          50.00     100.0000\t 91.11111%\t    89.83994% \n",
            "  10.000          50.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "  10.000         100.00       0.0001\t  3.33333%\t     0.00000% \n",
            "  10.000         100.00       0.0100\t  3.61111%\t     0.13918% \n",
            "  10.000         100.00       1.0000\t 18.88889%\t    16.77105% \n",
            "  10.000         100.00     100.0000\t 91.11111%\t    89.83994% \n",
            "  10.000         100.00   10000.0000\t 91.11111%\t    89.83994% \n",
            " 100.000          10.00       0.0001\t  3.61111%\t     3.13152% \n",
            " 100.000          10.00       0.0100\t  3.61111%\t     3.13152% \n",
            " 100.000          10.00       1.0000\t 18.88889%\t    16.77105% \n",
            " 100.000          10.00     100.0000\t 91.11111%\t    89.83994% \n",
            " 100.000          10.00   10000.0000\t 91.11111%\t    89.83994% \n",
            " 100.000          20.00       0.0001\t  4.16667%\t     1.67015% \n",
            " 100.000          20.00       0.0100\t  4.16667%\t     1.67015% \n",
            " 100.000          20.00       1.0000\t 18.88889%\t    16.77105% \n",
            " 100.000          20.00     100.0000\t 91.11111%\t    89.83994% \n",
            " 100.000          20.00   10000.0000\t 91.11111%\t    89.83994% \n",
            " 100.000          50.00       0.0001\t  3.61111%\t     0.00000% \n",
            " 100.000          50.00       0.0100\t  3.88889%\t     0.06959% \n",
            " 100.000          50.00       1.0000\t 18.88889%\t    16.77105% \n",
            " 100.000          50.00     100.0000\t 91.11111%\t    89.83994% \n",
            " 100.000          50.00   10000.0000\t 91.11111%\t    89.83994% \n",
            " 100.000         100.00       0.0001\t  3.33333%\t     0.00000% \n",
            " 100.000         100.00       0.0100\t  3.88889%\t     0.06959% \n",
            " 100.000         100.00       1.0000\t 18.88889%\t    16.77105% \n",
            " 100.000         100.00     100.0000\t 91.11111%\t    89.83994% \n",
            " 100.000         100.00   10000.0000\t 91.11111%\t    89.83994% \n",
            "\n",
            "el error de test mínimo ha sido de 2.222222% con un nos valores de C, iteraciones máximas y tolerancia de : 0.100, 100.00 y 0.000100\n",
            "el error de entrenamiento mínimo ha sido de 0.000000% con un nos valores de C, iteraciones máximas y tolerancia de : 1.000, 50.00 y 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kyJSBLyCLxZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sin-2324",
      "language": "python",
      "name": "sin-2324"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}